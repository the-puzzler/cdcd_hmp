{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from helper_funcs import generate_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6486\n",
      "Test size: 1622\n",
      "\n",
      "First few training samples:\n",
      "                                                            otu_arrays\n",
      "Unnamed: 0                                                            \n",
      "SRR044975.SRS011167  [30, 58, 82, 89, 93, 98, 99, 104, 117, 120, 12...\n",
      "SRR049604.SRS049164  [9, 10, 11, 14, 15, 16, 17, 20, 28, 30, 31, 32...\n",
      "SRR331714.SRS076947  [19, 30, 43, 58, 65, 70, 71, 74, 80, 90, 92, 9...\n",
      "SRR089999.SRS077685  [12, 14, 18, 20, 22, 38, 45, 67, 68, 76, 88, 1...\n",
      "SRR048091.SRS021563  [19, 30, 45, 52, 58, 60, 65, 70, 74, 80, 90, 9...\n",
      "\n",
      "Min array length: 3\n",
      "Max array length: 277\n",
      "Mean array length: 69.10\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "loaded_df = pd.read_hdf('./data/sample_otu_arrays.h5', key='df')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split indices into train/test\n",
    "train_idx, test_idx = train_test_split(loaded_df.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create train and test dataframes\n",
    "train_df = loaded_df.loc[train_idx]\n",
    "test_df = loaded_df.loc[test_idx]\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")\n",
    "print(\"\\nFirst few training samples:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Let's also look at array lengths\n",
    "array_lengths = [len(x) for x in loaded_df['otu_arrays']]\n",
    "print(f\"\\nMin array length: {min(array_lengths)}\")\n",
    "print(f\"Max array length: {max(array_lengths)}\")\n",
    "print(f\"Mean array length: {np.mean(array_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch tokens shape: torch.Size([8, 277])\n",
      "Batch mask shape: torch.Size([8, 277])\n",
      "\n",
      "Vocabulary size: 519\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class OTUDataset(Dataset):\n",
    "   def __init__(self, df):\n",
    "       self.df = df\n",
    "       \n",
    "       # Find max sequence length for padding\n",
    "       self.max_len = max(len(x) for x in df['otu_arrays'])\n",
    "       \n",
    "   def __len__(self):\n",
    "       return len(self.df)\n",
    "   \n",
    "   def __getitem__(self, idx):\n",
    "       # Get array for this sample\n",
    "       array = self.df.iloc[idx]['otu_arrays']\n",
    "       \n",
    "       # Create padded tensor\n",
    "       padded = torch.zeros(self.max_len, dtype=torch.long)\n",
    "       padded[:len(array)] = torch.tensor(array)\n",
    "       \n",
    "       # Create mask (False where we have real tokens, True for padding)\n",
    "       mask = torch.zeros(self.max_len, dtype=torch.bool)\n",
    "       mask[len(array):] = True\n",
    "       \n",
    "       return padded, mask\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = OTUDataset(train_df)\n",
    "test_dataset = OTUDataset(test_df)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify shapes\n",
    "for tokens, mask in train_loader:\n",
    "   print(f\"Batch tokens shape: {tokens.shape}\")\n",
    "   print(f\"Batch mask shape: {mask.shape}\")\n",
    "\n",
    "   break\n",
    "\n",
    "# Get vocab size (maximum token ID + 1 for padding)\n",
    "vocab_size = max(max(x) for x in loaded_df['otu_arrays']) + 1\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_funcs' from '/Users/matteo/Documents/MATLAS/full_model_and_SAE/flow_matching/helper_funcs.py'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model_arch\n",
    "import helper_funcs\n",
    "import importlib\n",
    "from model_arch import CategoricalScoreDiffusion\n",
    "from helper_funcs import generate_sequences\n",
    "importlib.reload(model_arch)\n",
    "importlib.reload(helper_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_cooccurrence_correlation(sequences, num_otus, reference_coocur=None, overall=False):\n",
    "   \"\"\"Calculate co-occurrence correlation for a set of sequences\"\"\"\n",
    "   matrix = np.zeros((len(sequences), num_otus-1))\n",
    "   for i, seq in enumerate(sequences):\n",
    "       unique_otus = set(otu for otu in seq if otu != 0)\n",
    "       for otu in unique_otus:\n",
    "           matrix[i, otu-1] = 1\n",
    "   \n",
    "   coocur = np.corrcoef(matrix.T)\n",
    "   \n",
    "   if reference_coocur is not None:\n",
    "       if overall:\n",
    "           mask = ~np.isnan(coocur) & ~np.isnan(reference_coocur)\n",
    "           correlation = np.corrcoef(coocur[mask], reference_coocur[mask])[0,1]\n",
    "       else:\n",
    "           all_otus = []\n",
    "           for seq in sequences:\n",
    "               all_otus.extend([x for x in seq if x != 0])\n",
    "           counts = Counter(all_otus)\n",
    "           top_otus = [otu-1 for otu, _ in counts.most_common(50)]\n",
    "           \n",
    "           top_coocur = coocur[top_otus][:, top_otus]\n",
    "           top_ref_coocur = reference_coocur[top_otus][:, top_otus]\n",
    "           mask = ~np.isnan(top_coocur) & ~np.isnan(top_ref_coocur)\n",
    "           correlation = np.corrcoef(top_coocur[mask], top_ref_coocur[mask])[0,1]\n",
    "       return correlation\n",
    "   \n",
    "   return coocur\n",
    "\n",
    "def ce_loss_simple(logits, target_tokens, temperature=0.1): #avg over seq then batch\n",
    "    B, S, V = logits.shape\n",
    "    logits_flat = logits.view(-1, V) / temperature\n",
    "    targets_flat = target_tokens.view(-1)\n",
    "    \n",
    "    # Create mask for non-pad tokens\n",
    "    mask = (targets_flat != 0).view(B, S)\n",
    "    \n",
    "    # Calculate CE loss per token\n",
    "    token_losses = F.cross_entropy(\n",
    "        logits_flat, \n",
    "        targets_flat, \n",
    "        reduction='none'\n",
    "    ).view(B, S)\n",
    "    \n",
    "    # Average over sequence first (using mask)\n",
    "    seq_lengths = mask.sum(dim=1)\n",
    "    sequence_loss = (token_losses * mask).sum(dim=1) / seq_lengths\n",
    "    \n",
    "    # Average over batch\n",
    "    loss = sequence_loss.mean()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmsd(generated_sequences, real_sequences, vocab_size):\n",
    "    def create_correlation_matrix(sequences):\n",
    "        # Create presence/absence matrix\n",
    "        matrix = np.zeros((len(sequences), vocab_size-1))\n",
    "        for i, seq in enumerate(sequences):\n",
    "            unique_otus = set(otu for otu in seq if otu != 0)\n",
    "            for otu in unique_otus:\n",
    "                matrix[i, otu-1] = 1\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        return np.corrcoef(matrix.T)\n",
    "    \n",
    "    # Get top OTUs from real sequences\n",
    "    all_otus = []\n",
    "    for seq in real_sequences:\n",
    "        all_otus.extend([x for x in seq if x != 0])\n",
    "    counts = Counter(all_otus)\n",
    "    top_otus = [otu-1 for otu, _ in counts.most_common(50)]\n",
    "    \n",
    "    # Calculate correlation matrices\n",
    "    real_matrix = create_correlation_matrix(real_sequences)\n",
    "    gen_matrix = create_correlation_matrix(generated_sequences)\n",
    "    \n",
    "    # Calculate RMSD for top OTUs only\n",
    "    real_subset = real_matrix[top_otus][:, top_otus]\n",
    "    gen_subset = gen_matrix[top_otus][:, top_otus]\n",
    "    return np.sqrt(np.mean((real_subset - gen_subset)**2))\n",
    "\n",
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_rmsd = float('inf')\n",
    "        \n",
    "    def update_best_metrics(self, val_loss):\n",
    "        improved = False\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            improved = True\n",
    "        # if rmsd < self.best_rmsd:\n",
    "        #     self.best_rmsd = rmsd\n",
    "        #     improved = True\n",
    "        return improved\n",
    "\n",
    "def train_step(model, tokens, mask, optimizer, device):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Sample time using warping\n",
    "    t = model.sample_time(tokens.shape[0], tokens.device)\n",
    "\n",
    "    # Get clean embeddings\n",
    "    x0 = model.embedding(tokens)\n",
    "  \n",
    "    \n",
    "    # Add noise\n",
    "    noise = model.get_noise(x0, t)\n",
    "\n",
    "    xt = x0 + noise\n",
    "\n",
    "    \n",
    "    # Get model predictions\n",
    "    logits = model(xt, mask, t)\n",
    "\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        tokens.view(-1),\n",
    "        ignore_index=0\n",
    "    )\n",
    "\n",
    "    if not torch.isnan(loss):\n",
    "        model.update_time_warping(t, loss.detach())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def validation_step(model, tokens, mask, device):\n",
    "    # Sample time using warping\n",
    "    t = model.sample_time(tokens.shape[0], tokens.device)\n",
    "    \n",
    "    # Get clean embeddings\n",
    "    x0 = model.embedding(tokens)\n",
    "    \n",
    "    # Add noise according to N(0, σt²)\n",
    "    noise = model.get_noise(x0, t)\n",
    "    xt = x0 + noise\n",
    "    \n",
    "    # Get model predictions\n",
    "    logits = model(xt, mask, t)\n",
    "    \n",
    "    # Compute cross-entropy loss with padding handling\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        tokens.view(-1),\n",
    "        ignore_index=0  # Assuming 0 is padding token\n",
    "    )\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        \n",
    "    }\n",
    "    torch.save(checkpoint, 'best_model.pt')\n",
    "\n",
    "def log_metrics(metrics_dict, step_type='batch'):\n",
    "    wandb.log(metrics_dict)\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (tokens, mask) in enumerate(train_bar):\n",
    "        tokens = tokens.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        loss = train_step(model, tokens, mask, optimizer, device)\n",
    "        train_loss += loss\n",
    "        \n",
    "        train_bar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "        log_metrics({\n",
    "            'train/batch_loss': loss,\n",
    "            'train/learning_rate': optimizer.param_groups[0]['lr'],\n",
    "            'epoch': epoch,\n",
    "            'batch': batch_idx\n",
    "        })\n",
    "    \n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch(model, test_loader, device, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_bar = tqdm(test_loader, desc=f'Validation Epoch {epoch}')\n",
    "    \n",
    "    # Collect real sequences\n",
    "    real_sequences = []\n",
    "    with torch.no_grad():\n",
    "        for tokens, mask in val_bar:\n",
    "            tokens = tokens.to(device)\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            loss = validation_step(model, tokens, mask, device)\n",
    "            val_loss += loss\n",
    "            val_bar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "            \n",
    "            real_sequences.extend([seq[seq != 0].cpu().numpy() for seq in tokens])\n",
    "    \n",
    "    # Generate sequences and calculate RMSD\n",
    "    # generated_sequences = generate_sequences(model, num_sequences=500, temperature=1, num_steps=20)\n",
    "    # rmsd = calculate_rmsd(generated_sequences, real_sequences, model.vocab_size)\n",
    "    \n",
    "    return val_loss / len(test_loader)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_validate(model, train_loader, test_loader, optimizer, num_epochs, device, use_lr_scheduling=True):\n",
    "    metrics = TrainingMetrics()\n",
    "    \n",
    "    scheduler = None\n",
    "    if use_lr_scheduling:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=3, factor=0.5, verbose=True\n",
    "        )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        avg_train_loss = train_epoch(model, train_loader, optimizer, device, epoch)\n",
    "        log_metrics({'train/epoch_loss': avg_train_loss, 'epoch': epoch})\n",
    "         \n",
    "        # Validation phase (every 5 epochs)\n",
    "        if epoch % 5 == 0:\n",
    "            avg_val_loss = validate_epoch(model, test_loader, device, epoch)\n",
    "            \n",
    "            log_metrics({\n",
    "                'val/epoch_loss': avg_val_loss,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "            \n",
    "            print(f'\\nEpoch {epoch}:')\n",
    "            print(f'Average Train Loss: {avg_train_loss:.4f}')\n",
    "            print(f'Average Val Loss: {avg_val_loss:.4f}')\n",
    "         \n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.step(avg_val_loss)\n",
    "            \n",
    "            if metrics.update_best_metrics(avg_val_loss):\n",
    "                save_checkpoint(model, optimizer, scheduler, epoch, avg_train_loss, avg_val_loss)\n",
    "                log_metrics({\n",
    "                    'best_model/val_loss': avg_val_loss,\n",
    "                    'best_model/train_loss': avg_train_loss,\n",
    "                    'best_model/epoch': epoch\n",
    "                })\n",
    "        else:\n",
    "            print(f'\\nEpoch {epoch}: Average Train Loss: {avg_train_loss:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "embed_dim = 16 #8 \n",
    "num_layers = 3 #5\n",
    "num_heads = 4\n",
    "dim_feedforward = 16 #32\n",
    "num_fourier_features = 8# going from 4 to 8 destabilised the batch loss but seems o have resulted in a faster convergence and lower\n",
    "model = CategoricalScoreDiffusion(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    num_fourier_features=num_fourier_features\n",
    "    \n",
    ")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Move model to device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▃▆▂▃▄▄▅▅▆▃▃▅▆▆▂▃▃▄▅▇▇▅▆▁▃▅█▂▂▄▆▆▇█▅▅▆▁▃</td></tr><tr><td>best_model/epoch</td><td>▁█</td></tr><tr><td>best_model/train_loss</td><td>█▁</td></tr><tr><td>best_model/val_loss</td><td>█▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/batch_loss</td><td>█▆▅▄▃▃▃▄▄▄▃▃▂▃▃▂▃▃▂▄▂▄▃▃▃▂▂▃▃▃▄▁▂▃▁▃▂▃▂▂</td></tr><tr><td>train/epoch_loss</td><td>█▃▂▂▂▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/epoch_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>390</td></tr><tr><td>best_model/epoch</td><td>5</td></tr><tr><td>best_model/train_loss</td><td>4.4706</td></tr><tr><td>best_model/val_loss</td><td>4.43708</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>train/batch_loss</td><td>4.07089</td></tr><tr><td>train/epoch_loss</td><td>4.4276</td></tr><tr><td>train/learning_rate</td><td>0.001</td></tr><tr><td>val/epoch_loss</td><td>4.43708</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comic-snow-37</strong> at: <a href='https://wandb.ai/matteopeluso1922/diffusion-hmp/runs/onkxd9u6' target=\"_blank\">https://wandb.ai/matteopeluso1922/diffusion-hmp/runs/onkxd9u6</a><br> View project at: <a href='https://wandb.ai/matteopeluso1922/diffusion-hmp' target=\"_blank\">https://wandb.ai/matteopeluso1922/diffusion-hmp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250114_163915-onkxd9u6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/matteo/Documents/MATLAS/full_model_and_SAE/flow_matching/wandb/run-20250114_164326-e7td2260</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matteopeluso1922/diffusion-hmp/runs/e7td2260' target=\"_blank\">sage-bee-38</a></strong> to <a href='https://wandb.ai/matteopeluso1922/diffusion-hmp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matteopeluso1922/diffusion-hmp' target=\"_blank\">https://wandb.ai/matteopeluso1922/diffusion-hmp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matteopeluso1922/diffusion-hmp/runs/e7td2260' target=\"_blank\">https://wandb.ai/matteopeluso1922/diffusion-hmp/runs/e7td2260</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matteopeluso1922/diffusion-hmp/runs/e7td2260?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1f8dbc910>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-2\n",
    "\n",
    "wandb.finish()\n",
    "wandb.init(\n",
    "    project=\"diffusion-hmp\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"restart\",\n",
    "        \"dataset\": \"hmp\",\n",
    "        \"epochs\": num_epochs,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"dim_feedforward\": dim_feedforward,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"num_fourier_features\":num_fourier_features\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 811/811 [00:28<00:00, 28.54it/s, loss=4.3285]\n",
      "Validation Epoch 0: 100%|██████████| 203/203 [00:02<00:00, 93.38it/s, loss=2.6559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:\n",
      "Average Train Loss: 4.1879\n",
      "Average Val Loss: 3.9029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 811/811 [00:27<00:00, 29.67it/s, loss=3.9660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Average Train Loss: 3.8453\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 811/811 [00:26<00:00, 30.26it/s, loss=3.3252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Average Train Loss: 3.8384\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 811/811 [00:27<00:00, 29.49it/s, loss=4.0613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Average Train Loss: 3.8035\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 811/811 [00:26<00:00, 30.39it/s, loss=4.3854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Average Train Loss: 3.8678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 811/811 [00:26<00:00, 30.57it/s, loss=3.8581]\n",
      "Validation Epoch 5: 100%|██████████| 203/203 [00:01<00:00, 104.51it/s, loss=4.5168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:\n",
      "Average Train Loss: 3.8411\n",
      "Average Val Loss: 3.8843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 811/811 [00:26<00:00, 30.45it/s, loss=4.3109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Average Train Loss: 3.8198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 811/811 [00:26<00:00, 30.39it/s, loss=3.6409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Average Train Loss: 3.8367\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 811/811 [00:26<00:00, 30.36it/s, loss=3.0007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Average Train Loss: 3.7981\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 811/811 [00:26<00:00, 30.60it/s, loss=3.7241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Average Train Loss: 3.8294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 811/811 [00:26<00:00, 30.57it/s, loss=4.5010]\n",
      "Validation Epoch 10: 100%|██████████| 203/203 [00:01<00:00, 105.74it/s, loss=3.0820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:\n",
      "Average Train Loss: 3.7893\n",
      "Average Val Loss: 3.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 811/811 [00:26<00:00, 30.56it/s, loss=2.9555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Average Train Loss: 3.8750\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 811/811 [00:26<00:00, 30.57it/s, loss=4.1427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: Average Train Loss: 3.8095\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 811/811 [00:26<00:00, 30.43it/s, loss=2.8313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: Average Train Loss: 3.8243\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 811/811 [00:26<00:00, 30.56it/s, loss=3.5406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: Average Train Loss: 3.7983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 811/811 [00:26<00:00, 30.54it/s, loss=4.2893]\n",
      "Validation Epoch 15: 100%|██████████| 203/203 [00:01<00:00, 106.02it/s, loss=3.7157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:\n",
      "Average Train Loss: 3.8074\n",
      "Average Val Loss: 3.8003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 811/811 [00:26<00:00, 30.47it/s, loss=4.1837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: Average Train Loss: 3.7768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 811/811 [00:26<00:00, 30.18it/s, loss=3.8816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: Average Train Loss: 3.7923\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 811/811 [00:27<00:00, 29.32it/s, loss=3.7803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Average Train Loss: 3.7400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 811/811 [00:26<00:00, 30.15it/s, loss=3.4114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Average Train Loss: 3.7644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 811/811 [00:26<00:00, 30.28it/s, loss=4.1232]\n",
      "Validation Epoch 20: 100%|██████████| 203/203 [00:01<00:00, 103.07it/s, loss=3.6387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "Average Train Loss: 3.7924\n",
      "Average Val Loss: 3.8110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 811/811 [00:26<00:00, 30.28it/s, loss=4.4074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: Average Train Loss: 3.7467\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 811/811 [00:27<00:00, 29.47it/s, loss=2.8486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: Average Train Loss: 3.7733\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 811/811 [00:26<00:00, 30.18it/s, loss=4.6399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: Average Train Loss: 3.7489\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 811/811 [00:27<00:00, 29.59it/s, loss=2.9454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: Average Train Loss: 3.7736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 811/811 [00:28<00:00, 28.80it/s, loss=3.3630]\n",
      "Validation Epoch 25: 100%|██████████| 203/203 [00:01<00:00, 103.95it/s, loss=4.3329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:\n",
      "Average Train Loss: 3.7805\n",
      "Average Val Loss: 3.7512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 811/811 [00:27<00:00, 29.38it/s, loss=3.3836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: Average Train Loss: 3.7760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 811/811 [00:27<00:00, 28.98it/s, loss=2.9961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: Average Train Loss: 3.7691\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 811/811 [00:27<00:00, 29.43it/s, loss=3.5246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: Average Train Loss: 3.7493\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 811/811 [00:26<00:00, 30.82it/s, loss=4.3527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: Average Train Loss: 3.7478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 811/811 [00:27<00:00, 29.68it/s, loss=2.8524]\n",
      "Validation Epoch 30: 100%|██████████| 203/203 [00:02<00:00, 99.95it/s, loss=4.0479] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:\n",
      "Average Train Loss: 3.7154\n",
      "Average Val Loss: 3.6752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 31: 100%|██████████| 811/811 [00:27<00:00, 29.98it/s, loss=4.3475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: Average Train Loss: 3.7783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 32: 100%|██████████| 811/811 [00:27<00:00, 29.49it/s, loss=4.8974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: Average Train Loss: 3.8022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 33: 100%|██████████| 811/811 [00:27<00:00, 29.35it/s, loss=4.4593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: Average Train Loss: 3.7485\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 34: 100%|██████████| 811/811 [00:26<00:00, 30.34it/s, loss=3.8310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: Average Train Loss: 3.7151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 35: 100%|██████████| 811/811 [00:27<00:00, 30.01it/s, loss=3.1949]\n",
      "Validation Epoch 35: 100%|██████████| 203/203 [00:01<00:00, 105.82it/s, loss=3.9745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:\n",
      "Average Train Loss: 3.7853\n",
      "Average Val Loss: 3.6613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 36: 100%|██████████| 811/811 [00:26<00:00, 30.53it/s, loss=3.8548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36: Average Train Loss: 3.7752\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 37: 100%|██████████| 811/811 [00:26<00:00, 30.19it/s, loss=1.4777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: Average Train Loss: 3.7706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 38: 100%|██████████| 811/811 [00:27<00:00, 29.41it/s, loss=2.6082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: Average Train Loss: 3.7580\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 39: 100%|██████████| 811/811 [00:26<00:00, 30.14it/s, loss=4.7430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: Average Train Loss: 3.7151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 40: 100%|██████████| 811/811 [00:26<00:00, 30.61it/s, loss=4.8232]\n",
      "Validation Epoch 40: 100%|██████████| 203/203 [00:01<00:00, 104.97it/s, loss=3.0023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:\n",
      "Average Train Loss: 3.7620\n",
      "Average Val Loss: 3.7332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 41: 100%|██████████| 811/811 [00:26<00:00, 30.55it/s, loss=3.5530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: Average Train Loss: 3.7270\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 42: 100%|██████████| 811/811 [00:26<00:00, 30.68it/s, loss=4.3687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: Average Train Loss: 3.7591\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 43: 100%|██████████| 811/811 [00:26<00:00, 30.50it/s, loss=4.5337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: Average Train Loss: 3.7186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 44: 100%|██████████| 811/811 [00:26<00:00, 30.55it/s, loss=4.7582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: Average Train Loss: 3.7288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 45: 100%|██████████| 811/811 [00:26<00:00, 30.55it/s, loss=2.3390]\n",
      "Validation Epoch 45: 100%|██████████| 203/203 [00:01<00:00, 105.88it/s, loss=3.0325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:\n",
      "Average Train Loss: 3.7324\n",
      "Average Val Loss: 3.7468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 46: 100%|██████████| 811/811 [00:27<00:00, 29.60it/s, loss=4.3324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: Average Train Loss: 3.7409\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 47: 100%|██████████| 811/811 [00:28<00:00, 28.10it/s, loss=4.3845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: Average Train Loss: 3.7321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 48: 100%|██████████| 811/811 [00:33<00:00, 24.28it/s, loss=5.1703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: Average Train Loss: 3.7429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 49: 100%|██████████| 811/811 [00:35<00:00, 23.03it/s, loss=4.3174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: Average Train Loss: 3.7453\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 50: 100%|██████████| 811/811 [00:31<00:00, 25.61it/s, loss=3.3565]\n",
      "Validation Epoch 50: 100%|██████████| 203/203 [00:02<00:00, 98.19it/s, loss=3.8315] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50:\n",
      "Average Train Loss: 3.7017\n",
      "Average Val Loss: 3.7542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 51: 100%|██████████| 811/811 [00:26<00:00, 30.13it/s, loss=3.2753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: Average Train Loss: 3.7778\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 52: 100%|██████████| 811/811 [00:27<00:00, 29.73it/s, loss=3.8160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52: Average Train Loss: 3.7088\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 53:  49%|████▉     | 396/811 [00:13<00:14, 29.43it/s, loss=3.2663]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 175\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, test_loader, optimizer, num_epochs, device, use_lr_scheduling)\u001b[0m\n\u001b[1;32m    169\u001b[0m     scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m    170\u001b[0m         optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     avg_train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     log_metrics({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/epoch_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch})\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# Validation phase (every 5 epochs)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[130], line 125\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m    122\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    123\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 125\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    128\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m})\n",
      "Cell \u001b[0;32mIn[130], line 73\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, tokens, mask, optimizer, device)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n\u001b[1;32m     72\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate_time_warping(t, loss\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/matlas/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matlas/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Start training\n",
    "train_and_validate(model, train_loader, test_loader, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'model_args': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embed_dim': embed_dim,\n",
    "        'num_layers': num_layers,\n",
    "        'num_heads': num_heads,\n",
    "        'dim_feedforward': dim_feedforward,\n",
    "        'num_fourier_features': num_fourier_features\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'model_checkpoint_3.58pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_arch import CategoricalScoreDiffusion\n",
    "\n",
    "checkpoint = torch.load('model_checkpoint_2.65.pt')\n",
    "model = CategoricalScoreDiffusion(**checkpoint['model_args'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Access the learning rate\n",
    "# Get the optimizer state dict\n",
    "optimizer_state = checkpoint['optimizer_state_dict']\n",
    "learning_rate = optimizer_state['param_groups'][0]['lr']\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
